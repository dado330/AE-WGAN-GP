[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AE-WGAN-GP",
    "section": "",
    "text": "1 Introduction\nSome people say that we are currently living in the Information Age.\nIn many fields there is nowadays a necessity to have representative synthetic data for multiple reasons:\n\nBy simulating actual data characteristics and patterns, they are able to mimic real-world scenarios correctly. Such sets empower researchers to query the dataset and expect an accurate response by impersonating features and trends of true data\nThey help researchers overcome limitations and ethical concerns that are usually associated with using real data. At times, privacy, security or legal restrictions could limit access to authentic data. By utilizing synthetic data these concerns can be alleviated.\n\nThe path I took to develop the AE-WGAN-GP model from scratch has been quite a journey.\nI will attempt to replicate the informal style and tone found in some medium post.\nIn the first chapter I will explain the general idea I had in the beginning and the preprocessing of the data.\nThe second chapter talks about the AutoEncoder\nIn the third chapter I describe how I setup Colab and Git\nFinally in the last chapter contains everything regarding the GAN"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Some people say that we are currently living in the Information Age.\nIn many fields there is nowadays a necessity to have representative synthetic data for multiple reasons:\n\nBy simulating actual data characteristics and patterns, they are able to mimic real-world scenarios correctly. Such sets empower researchers to query the dataset and expect an accurate response by impersonating features and trends of true data\nThey help researchers overcome limitations and ethical concerns that are usually associated with using real data. At times, privacy, security or legal restrictions could limit access to authentic data. By utilizing synthetic data these concerns can be alleviated.\n\nThe path I took to develop the AE-WGAN-GP model from scratch has been quite a journey.\nI will attempt to replicate the informal style and tone found in some medium post.\nIn this chapter I will explain the general idea I had in the beginning and the preprocessing of the data."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Choi, Edward, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F.\nStewart, and Jimeng Sun. 2018. “Generating Multi-Label Discrete\nPatient Records Using Generative Adversarial Networks,” no.\narXiv:1703.06490 (January). https://doi.org/10.48550/arXiv.1703.06490.\n\n\nGulrajani, Ishaan, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron Courville. 2017. “Improved Training of Wasserstein\nGANs.” https://arxiv.org/abs/1704.00028.\n\n\nThurin et, al. 2022. “From Inception to ConcePTION: Genesis of a\nNetwork to Support Better Monitoring and Communication of Medication\nSafety During Pregnancy and Breastfeeding.” Clinical\nPharmacology & Therapeutics 111 (1): 321–31. https://doi.org/10.1002/cpt.2476."
  },
  {
    "objectID": "medgan.html",
    "href": "medgan.html",
    "title": "2  MedGAN",
    "section": "",
    "text": "The original MedGAN article (Choi et al. 2018) was written by Choi et al. in 2017 and descrived for the first time a way to generate realistic synthetic patient records.\n The idea was simple train: an autoencoder on the original data and then use its decoder as part of the generator of a GAN network to gnerate synthetic data.\nMy initial idea was to modify the input of the network proposed in the original paper to ingest my dataset, maybe play a little with activation functions and optimizers to find the optimal implementation for my use case.\n\n\n\nBoy, was I wrong!\n\n\n\n\n\n\nChoi, Edward, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun. 2018. “Generating Multi-Label Discrete Patient Records Using Generative Adversarial Networks,” no. arXiv:1703.06490 (January). https://doi.org/10.48550/arXiv.1703.06490."
  },
  {
    "objectID": "cdm_and_instance.html",
    "href": "cdm_and_instance.html",
    "title": "3  CDM and Data instance",
    "section": "",
    "text": "ConcePTION(see al. 2022) is an IMI project which began in 2019. As one of the project outputs, ConcePTION aimed to establish a trusted ecosystem that generates and disseminates reliable, evidence-based information regarding effects of medication used during pregnancy and breastfeeding. To this end, a CDM was designed to manage, within constrained timelines and budget, the heterogeneity inherent in the diverse data sources in Europe.\nThe ConcePTION CDM includes 16 tables. Each table includes multiple variables. The picture below summarises the ConcePTION CDM.\nAs you can see, the tables have four different colours. The colours indicate the four types of data tables:\n\nGreen: Routine healthcare data, such as data related to medical events (diagnoses/symptoms), medicines, vaccines, medical procedures and medical observations.\nDark blue: Surveillance data, such as registries (birth registries, congenital anomaly registries, disease registries, …), surveys or cohorts.\nLight blue: Curated data, such as demographics, observation periods and person relationships (e.g., mother/child linkage).\nGrey: Metadata, i.e., information regarding the data in the model, such as extraction dates and drugs detailed definition.\n\nLinkages between the different tables are represented by lines:\n\nSolid black lines: The linkages across records of the same person (e.g., patient demographics from Persons table to diagnosis codes from Events table).\nDotted lines: The linkages across items extracted from the same record (e.g., hospital stays from Visit occurrence table to diagnosis codes from Events table).\nSolid grey lines: The linkages from items referring to a medicinal product or vaccines to the full product description in the Product table.\n\n\nA data instance of a CDM refers to a set of structured data that conforms to the predefined schema and semantics.\nTo train the AE-WGAN-GP I used synthetic data since for privacy concerns I could used real data.\nThe instance I used is available here.\n\n\n\n\nal., Thurin et. 2022. “From Inception to ConcePTION: Genesis of a Network to Support Better Monitoring and Communication of Medication Safety During Pregnancy and Breastfeeding.” Clinical Pharmacology & Therapeutics 111 (1): 321–31. https://doi.org/10.1002/cpt.2476."
  },
  {
    "objectID": "preprocessing_using_r.html",
    "href": "preprocessing_using_r.html",
    "title": "4  Preprocessing using R",
    "section": "",
    "text": "In the previous section I explained how a data instance of the ConcePTION CDM is a complex object.\nDefining a model which would generate an entire instance is outside the scope of this project and even SOTA models have not arrived there yet.\nUsing an R script I firstly extracted all conceptsets defined in the codelist available here\nI then created variables based on those conceptsets and finally a dataset which contains records of persons positive to 18 certain events in the year prior to a specific moment. A person obviously might have more than one event in that period of time.\nSome of the more rare events are being excluded since the dataset size is small and training the model would be too complicated if those would have been retained.\nThese are the final dataset characteristic:\n\n719 rows\n18 columns representing the 18 final events:\n\nB_COAGDIS_AESI\nC_ARRH_AESI\nC_CAD_AESI\nC_MYOCARD_AESI\nC_VALVULAR_AESI\nDEATH\nD_PANCRACUTE_AESI\nE_DM1_AESI\nE_GOUT_AESI\nG_KIACUTE_AESI\nG_UTI_AESI\nI_INFLUENZA_AESI\nM_FRACTURES_AESI\nM_OSTEOARTHRITIS_AESI\nN_STROKEHEMO_AESI\nSO_OTITISEXT_AESI\nV_THROMBOSISARTERIALALGOR_AESI\nV_VTEALGORITHM_AESI\n\nAll 18 columns are binary variable with 1 =&gt; the person had the event, 0 =&gt; otherwise\n28 records with 3 events, 52 records with 2 events and the remaining with 1 event\nSome rare event, f.e. N_STROKEHEMO_AESI with 10 records\nA very common event which is DEATH with 258 records\n\nTo finish the preprocessing I transformed the dataset to Numpy array ready to be ingested in Python"
  },
  {
    "objectID": "local_env.html",
    "href": "local_env.html",
    "title": "5  Local environment setup",
    "section": "",
    "text": "First thing I needed to do after having generated the input data for the network was to install python.\nI went to Python website, downloaded the latest release and then installed it. After installation I updated the Windows PATH and upgrade pip from CMD.\nI then installed PyCharm as IDE through a student license and proceed to install all necessary packages. Especially TensorFlow.\nHere, I got my first error:\n\nCould not find a version that satisfies the requirement TensorFlow (from versions: ) No matching distribution found for TensorFlow.\n\nAfter digging in Stack Overflow I found what the problem really was: TensorFlow was not yet available for the latest python version. I then downgraded python and reinstalled all packages, TensorFlow included this time.\nAfter running a sample TensorFlow script in noticed two messages that appears every time I would import the module\n\nI tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable TF_ENABLE_ONEDNN_OPTS=0.\nI tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\nAfter a little bit of research I found the first one was not very important.\nThe second one is a suggestion to use optimized instruction sets even not in performance-critical operations. I left this one as is because I first wanted to have a feel on the training time and as a last resort compile TensorFlow to take advantage of those instruction sets. In addition my CPU is a fairly old I3-3220 without even AVX2 support so I thought the difference would be negligible anyway."
  },
  {
    "objectID": "why_ae_in_gan.html",
    "href": "why_ae_in_gan.html",
    "title": "6  Why an AutoEncoder in a GAN?",
    "section": "",
    "text": "In general the main advantage of AutoEncoders (AE) is the dimensionality reduction of its input. This compact representation then force the network to learn the salient features of the initial data.\nHowever using an autoencoder in combination with a GAN has several additional benefits which in my opinion are more important:\n\nHelp Generator Training: The generator in a GAN learns to generate realistic samples by fooling the discriminator. The autoencoder acts as a pre-training mechanism for the generator since it won’t need to learn to generate discrete data but only salient features for the decoder.\nRegularization: Incorporating an autoencoder as part of a GAN architecture can act as a regularization technique since it acts as an additional constraint for the generator by forcing it to generate samples that can be accurately reconstructed. This helps in avoiding mode collapse.\n\nMore on this topics later on in the GAN chapter."
  },
  {
    "objectID": "original_code.html",
    "href": "original_code.html",
    "title": "7  Original code is unusable",
    "section": "",
    "text": "After installing Python and TensorFlow I was ready to run some code.\nThe MedGAN paper provides the souce code in a GitHub repository here and my first idea was to reuse that.\nI cloned the repo on my local machine and I started changing it to be able to ingest my initial data. I removed the CMD listener found in the script and transform my R data into a Numpy matrix.\nFinally I tried running the script but I soon found a sad reality: the script was built using TensorFlow V1.\nI tried disabling eager execution but to no avail: I was then made aware that to run this script I would have needed to update possibly many of its functions to their newer V2 versions which might be retroactively compatible.\n\n\n\nThis is what I feared would have happened\n\n\nI then decided to find another, more recent, implementation of the MedGAN network, possibly implemented by a third party.\nI searched trough forks with updated code but to no avail.\nI then made an extensive search on GitHub and found this repository: Benchmarking framework of synthetic electronic health record generation models.\nInside the repo there is a file containing the MedGAN network and I decided it would have been easier to update that script since it was much more recent than the previous one."
  },
  {
    "objectID": "fix_github.html",
    "href": "fix_github.html",
    "title": "8  Fixing what found in GitHub",
    "section": "",
    "text": "After cloning the new repo I tried analyzing the script to understand what it was doing and the reason behind each actions.\nI made a few observation and changes:\n\nThe input data in this case was not a Numpy matrix but a series of Numpy arrays.\nI then changed accordingly the procedure to transform the R data in the Numpy object.\nThe original paper works with count or binary variables (XOR) while this script utilizes both at the same times./ I’m going to discuss this topic in the AE and WGAN-GP sections.\nThe class AdamWeightDecay defined in the script was not working correctly so I just switched to AdamW function defined in tf.keras.optimizers since it didn’t seem to me the original class was doing anything special.\nThe argument interpolation of the function Numpy.nanpercentile() has been deprecated so I changed to method after looking at the documentation online.\nThe script was creating 5 different models. I only needed one so I removed a cycle.\nChanged paths of both the input data and output files\n\nAfter this I was ready to train the MedGAN on my data (or so I thought…)"
  },
  {
    "objectID": "AE_first_time.html",
    "href": "AE_first_time.html",
    "title": "9  Running the AutoEncoder for the first time",
    "section": "",
    "text": "Above is pretty much my reaction when I trained for the first time the AE. The loss did converges but it wasn’t near zero at all so the model was not learning properly.\nTo help visualizing what was going on I added a piece of code that would print the current batch, synthetic and then real data.\n1if np.random.uniform(size=1) &gt; 0.99:\n2  print(\"synthetic\")\n  print(tf.round(synthetic).numpy())\n3  print(\"real\")\n  print(real.numpy())\n\n1\n\nEvery 1 out 100 batches do\n\n2\n\nPrint the reconstructed data\n\n3\n\nPrint the original data\n\n\nI then noticed how the generated data initially had more frequent events as 1 for all of the records then after some time they suddenly become 0 one after the other. This was clearly not ideal."
  },
  {
    "objectID": "encoder_activation.html",
    "href": "encoder_activation.html",
    "title": "10  Activations of output layer of Encoder",
    "section": "",
    "text": "I started reviewing the Encoder and the Decoder to find macroscopic mistakes or something wrong at a glance. I didn’t noticed anything in particular so I started reading the MedGAN paper to look for clues.\nI subsequently noticed how the activation function of the output layer I used for the Encoder was a sigmoid while in the paper was a hyperbolic tangent (Tanh).\nI changed it and the loss decreased from 800~1000 to around 500~700. All the values were still 0, they were just converging to 0 faster probably.\nIt’s almost always preferable to use Tanh instead of sigmoid as action function whenever it is possible. An example of when it is better to use the sigmoid is in our case as activation function of the Decoder’s output layer: each value of our original data (so our ideal output) is 0 or 1, since the sigmoid output values from 0 to 1 this is a perfect fit. On the other hand it is not important what is the output of the Encoder. The Encoder map the original input onto a latent space which it is not necessary to be restricted to the range [0, 1]\nThe reason the Tanh is in general better than the sigmoid is simple: the outputs of the Tanh are zero-centered.\nThis translates in easier and more stable training for the model.\nObviously the model as of right now it’s still useless so I still continued to fine tune the network."
  },
  {
    "objectID": "AE_hyperparameters.html",
    "href": "AE_hyperparameters.html",
    "title": "11  Hyperparameters",
    "section": "",
    "text": "I then started improving the model by carefully testing combinations of hyperparameters because:\n\nI’ve tried multiple times to changes parameters some of them are:\n\nNumber of models trained:\nMaybe the model is not stable and every instance may result in very different final losses.\nIncrease layers size and networks depth:\nThe Encoder and Decoder may not have the capabilities to learn correctly because there aren’t sufficient nodes/connections between them.\nDecreasing learning rate γ, β1 or β2:\nIf the model is learning too quickly it might be overshooting and the loss may be bouncing around its minimum without really decreasing.\nIncreasing learning rate γ, β1 or β2:\nIf the model is learning too slowly it can get stuck in a local minima.\nChange batch size:\nLess important than previous points. In theory larger batches should decrease learning rate but make the model more consistent, the other way around for smaller batches. No real changes in my opinion in this case. I didn’t really think this would have been a solution since the model was already consistent but converging to a not good enough state.\nBatch normalization:\nCentering and scaling the input of a layer should make the model converge faster and more stable.\nActivation function of inner layers (and Batch normalization):\nI tried using Selu as activation function instead of Relu to add internal normalization and to not remove negative weights.\n\nIn the sections I will explain what worked in my case."
  },
  {
    "objectID": "expand_models.html",
    "href": "expand_models.html",
    "title": "12  Expanding the models",
    "section": "",
    "text": "At this point I was quite desperate to have something at least usable so I decide to increase the size of the Encoder and Decoder.\nI left both the networks with 3 hidden layers ans started increasing the size of each layer.\nAt the beginning the Encoder had layers with 18, 32, 64, 128, 256 units while the Decoder 256, 128, 64, 32, 18.\nI then begin to double the size of each layer in hope this would help the model train (except the 18 which is fixed)\nAs a summary:\n\nMaximum size 256, loss: 500~700\nMaximum size 512, loss: 500\nMaximum size 1024, loss: 500\nMaximum size 2048, loss: 500\nMaximum size 4096, loss: 500\n\nThe loss remain pretty much the same, just more consistent and faster in reaching the lower bound found in more parsimonious models. Everything is still 0."
  },
  {
    "objectID": "custom_loss.html",
    "href": "custom_loss.html",
    "title": "13  Custom loss",
    "section": "",
    "text": "Following the paper I originally used the binary cross-entropy as loss function of the AutoEncoder. At this point, however, I had the idea to modify it to better suit my data.\nAfter some thought my hypothesis was:\n\nThe dataset is sparse. There are only a few “1” values with respect to “0” values.\nThis problem might be exacerbated by the sparse nature of the more uncommon events.\nThe loss which is the Binary Cross Entropy in theory should handle class imbalance but maybe not on sparse data.\n\nI tried then to think about a possible solution:\n\nThe loss is generated by values which are 0 but should be 1, never the other way around\nMaybe the loss is not penalizing enough the case when 0 should be a 1\nWhich function can penalizes values near 0 but not near 1? The Log is a good candidate\nI first tried to take the negative Log of the syntetic data and obviously everything degenerated to 1\nI then thought to compare the synthetic and real data: if they are both the same then the loss should be 0\n\nMy final solution was:\n\\[\n- Ln(mean(synthetic + e^{-10})) + Ln(mean(real))\n\\]\nThis formula penalizes when the sum of the synthetic data is lower than the one on real data. The loss is especially larger when the sum of the synthetic data is near 0.\nThere are more refined approaches to this problem but this seemed to work.\n(Note: I needed to add the exponential to make the model more robust. Otherwise in case the syntethic data in the batch was all 0 at any point it would be impossible for the loss to be calculated)\nUsing this formula as a loss penalty in addition to the binary cross-entropy I was able to get the loss of the model with maximum size 4096 to around 200. Now the model partially encode and decode correctly some of the variables. Those variables are in general the more common events, the rarer events remain at 0.\nThis is still not sufficient and I tried different approaches in next sections"
  },
  {
    "objectID": "AE_activation.html",
    "href": "AE_activation.html",
    "title": "14  Activations of hidden layers of AE",
    "section": "",
    "text": "Both the Encoder and the Decoder used Relu as activation function of their hidden layers. Relu is easy to calculate but at the same time negative inputs turns to 0 instantaneously.\nUsing Tanh as activation function of the output layer of the Encoder means it is possible to have negative values as output so we might want to change the Relus to something else.\nI choosed to try leaky Relu but there were no major changes between this and Relu. I then decided to use Tanh as activation layer of the hidden layers too.\nThis change was very impactful and the loss became 0 very fast.\nUsing 4096 as maximum size of the model previously increased the computation time to around 40s so this may not be feasible when will train the GAN so I decided to train and decrease the size of the models.\nAs a summary:\n\nMaximum size 4096, loss: ~0\nMaximum size 2048, loss: ~0\nMaximum size 1024, loss: ~8 (but on test data ~20)\nMaximum size 512, loss: ~25 (but on test data ~50)\nMaximum size 256, loss: ~250\n\nTo get those loss values I needed to gradually increase then number of epochs, from 50 for the 4096 size to 1000 for the 256 size.\nIn an effort to increase performance I then implemented Batch Normalization (BN) after each hidden layer but without notable results.\nSince changing the activation function of the hidden layer has such a big impact I decided to make a new attempt.\nI tried to use more unusual activations and found Selu to be very good. This maybe be caused by the self-normalizing nature of the activation function which works better than BN in this case.\nWith Selu I could get Encoder and Decoder with maximum size 512 to have 0 loss and train much faster in less than 50 epochs compared to 400 with Tanh\nUsing 256 as maximum size still left me with around 40~80 loss.\nI thought of the possibility to change the activation function of the Encoder’s output layer too to Selu but I was unsure if that would creating problems in the GAN. The Selu didn’t seem symmetric so taking a sample from a Normal distribution might not be ideal (Increasing the difficulty in training the Generator)"
  },
  {
    "objectID": "general_param_AE.html",
    "href": "general_param_AE.html",
    "title": "15  Miscellaneous Hyperparameters",
    "section": "",
    "text": "As optimizers I left what was already there, so AdamW.\nI used different learning rates during the previous training but in general:\n\n0.0001 the first 3 epochs\n0.00006 between 4 and 10 epochs\n0.00003 between 11 and 20 epochs\n0.00001 after 21 epochs\n\nThe higher learning rate at the beginning is used to speed up the training while the lower learning rate at the end is to fine tune the model.\nI slightly decreased β1 to help lowering the learning rate.\nI decreased weight decay too, to be consistent with the lower learning rate."
  },
  {
    "objectID": "bad_vs_bad.html",
    "href": "bad_vs_bad.html",
    "title": "16  Bad model vs Bad model",
    "section": "",
    "text": "The model until now it’s working very well but it is because it clearly overfitting.\nI then tried to create models with lower or same units as the input but they couldn’t recreate the original data well.\nIt was necessary to decide if I should use the overfitting model (Model A) or the not accurate one (Model B)\n\n\n\nThe problem I was facing could be summarized by this image\n\n\nIn general it is better to use neither of them, obviously, but in our case which is the worse model?\nThe issue with the model A is that it is probably only capable of reconstructing data very similar to the training data.\nThe models B on the other hand don’t encode and decode the more uncommon event at all. They are treated as not existent.\nThe objective of the MedGAN (and AE-WGAN-GP) is to generate synthetic data similar to the original one. With model A the data will be very similar almost identical to the original while model B won’t create uncommon variables as they will always be 0.\nThis crucial in our case since in epidemiological studies we are often more interested in rare events since there is less information regarding them in the litereature.\nTaking into account the above mentioned reasons I decided to use model A, so the overfitting one."
  },
  {
    "objectID": "why_colab.html",
    "href": "why_colab.html",
    "title": "17  Why use Google",
    "section": "",
    "text": "This chapter explain my switch from PyCharm to Google Colab.\nIn reality this happen just after running the GAN on my PC. Before with the AE the maximum running time was around 40 minutes however the GAN surpassed the psychological threshold of 1 hour so I decided to find a faster solution.\n\nSearching a solution to optimize my code made me stumble on Google Colab. Google Colab is a free cloud-based Jupyter notebook environment that runs on Google infrastructure. It allows users to run Python scripts in a browser-based interface, with access to GPUs (with caveats).\n\n\n\nHow I felt after discovering Google Colab\n\n\nThat was perfect for me! In addition to have the possibility to run my models on a GPU I could already generate my plots and results on a Jupyter notebook which I could then share and Google Colab provides pre-installed libraries for ML and data analysis so no more problems with Python and libraries versions mess.\nI then started setting up the environment and understanding how to make my script works on Colab"
  },
  {
    "objectID": "setup_drive.html",
    "href": "setup_drive.html",
    "title": "18  Setup on Drive",
    "section": "",
    "text": "The first thing I did was copying what I already have done inside cells on Colab. I then uploaded my input dataset on the VM and modified the path inside the script.\nAfter a day has passed with me cleaning the script and rerunning the AutoEncoder on Colab I found out the VM state is not saved once you are disconnected from it.\nThe easiest solution and the one sponsored By Google is to use Google Drive.\nNext, I uploaded the input data on my Google Drive which is then mounted the VM on Colab using the following command (and by giving many permissions)\nfrom google.colab import drive\ndrive.mount(‘/content/gdrive’)\nI changed again the path to the input files (and checkpoints) and rerun again the AutoEncoder."
  },
  {
    "objectID": "setup_git.html",
    "href": "setup_git.html",
    "title": "19  Setup Git",
    "section": "",
    "text": "At some point I wanted to test the script on my local machine before running it in Colab (see next chapter and section) but copying the files was redundant and prone to errors.\nIn addition I discovered that once I shared my notebook with some else directly from Colab all outputs will be lost.\nThe solution to these problems I arrived was using Git, a distributed version control system.\nFirstly, I opened a GitHub repository and from my local machine I uploaded the preprocessing and the input data.\nOn the VM the process is slightly more difficult:\n\nI setup my credentials and path to GitHub. Credentials are to used only during the push to GitHub\n\nGIT_USERNAME = \"dado330\"\nGIT_MAIL = \"messina.davide.statistician@gmail.com\"\nGIT_REPO = \"https://github.com/dado330/AE-WGAN-GP\"\n\nI cloned the repository on the VM. I didn’t need any form of authentication since the repo is public.\n\n!git clone $GIT_REPO\n\nI inserted a cell to pull changes I’ve tested locally.\n\n%%bash\ncd /content/AE-WGAN-GP\ngit pull origin main\n\nThis cell makes Git aware of our credentials\n\n!git config --global user.email $GIT_MAIL\n!git config --global user.name $GIT_USERNAME\n\nTo push changes we need a token. You can easily generate one in your GitHub account with at least Repo scope. (For example: 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).\nThen set the origin where to push the commits to a combinations of token and repository link as below.\n\n%%bash\ncd /content/AE-WGAN-GP\ngit remote set-url origin \"https://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@github.com/dado330/AE-WGAN-GP.git\"\n\nHere I stage all changes. Commit them with a message and then push the commit to the origin defined previously.\n\n%%bash\ncd /content/AE-WGAN-GP\ngit add -A\ngit commit -m \"Updated comments and generated data\"\ngit push -u origin main\nThe Jupyter notebook was saved on the repository using the internal options of Google Colab.\nNow all files related to the projects are publicly accessible and included the notebook’s outputs."
  },
  {
    "objectID": "drawback_summary.html",
    "href": "drawback_summary.html",
    "title": "20  Drawbacks and final summary",
    "section": "",
    "text": "One of the reason to make a first run on my local machine before running the model on Colab was the time limit imposed the virtual machine instance by Google.\nAfter 12 hours the disconnection is automatic but it often happens before that limit if Google feel that you are not interacting with the session anymore.\n\n\n\nThis is how I felt\n\n\nIf I needed to be away from my PC I used a software to emulate key strokes, mouse movements and clicks.\nThe real issue when I was working and in the meantime the model was running on Colab in the background, a situation which happened most of the times.\nIn this situation it happened multiple times that my session disconnected while the model was still training.\nAnother reason to use Colab was to have access to a GPU to train my models.\nI discovered after testing that GANs do not really take advantage of the computational power of a GPU so I was quite bummed out. It was not uncommon to have a GPU instance slower than a CPU one.\nThere was also a huge variance between VM instances with respect to running time of the models: it could happen that opening a new VM would double the computation time of each epochs with changing any parameters.\nIn addition since the VM’s resources we are with are shared between multiple user on a single server there is a penalty in performance. The time it takes to run the model on Colab is comparable to what my PC is capable of using an I3 from more than 10 years ago and DDR3.\nTo summarize my experience with google Colab I would say:\nPositive:\n\nPython environment already setup. No need check python or package dependencies/versions, GPU drivers, etc..\nUse Jupyter notebook\nDo not use local machine resources which can then be used for other tasks\nMachine has a discrete amount of RAM to run the model (more than the 16GB on my PC)\n\nNeutral:\n\nSharing the notebook from within Colab will lose all output but there is a solution by using Git\nThere is a penalty in the performance with respect to the same machine running the same model locally\n\nNegative:\n\nVariance in performance between VM instances of the same class\nDisconnections are possible even before the 12 hours limit\nOnce your session is disconnected all data not saved somewhere else are lost"
  },
  {
    "objectID": "GAN_first_time.html",
    "href": "GAN_first_time.html",
    "title": "21  Running the GAN for the first time",
    "section": "",
    "text": "Now it is time to run the GAN and try to really simulate our synthetic data.\nIn the previous chapter “Fixing what found in GitHub” I already discussed what I changed before the first run.\nIn addition I needed the changed the output of the Generator to be 512 units, in line with what is the input of the Decoder.\nI started running the model and these were my observation:\n\nAt the very beginning there a moderate decrease in Generator while the Discriminator stayed pretty much the same\nThe Generator loss continued to decrease and the Discriminator one started to increase\nThis trend was not linear in both networks\nAfter some time the Discriminator loss started to decrease and the Generator one to increase\nThe Discriminator loss converged to a very small number while the Generator loss converged to a value higher than the starting point\nI then tried to generate some synthetic data using the trained Generator but I discovered how it only generated observation with the event DEATH which is the most common one\nThe GAN was mode collapsing (see point 6)\n\nThe next section will explain mode collapsing."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "22  Results",
    "section": "",
    "text": "Using the Generator I created a dataset 1000 times larger than the initial one.\n\n\n\n10000 Times didn’t work…\n\n\nThe Generator is not perfect so some of its output contains only 0. Since that is clearly impossible I decided to remove those rows. In total they were less than 2% of the generated dataset.\nI then calculated the mean number of cases for each events I would get if I generated a dataset of the same size as the initial one\n\n\n\n\n\n\n\n\n\n\nVarnames\n0_real\n0_synthetic\ndiff_perc\n\n\n\n\n\nB_COAGDIS_AESI\n73.0\n73.715309\n-0.98\n\n\n\nC_ARRH_AESI\n65.0\n69.045921\n-6.22\n\n\n\nC_CAD_AESI\n31.0\n31.479591\n-1.55\n\n\n\nC_MYOCARD_AESI\n20.0\n17.986734\n10.07\n\n\n\nC_VALVULAR_AESI\n50.0\n51.130611\n-2.26\n\n\n\nDEATH\n258.0\n253.684692\n1.67\n\n\n\nD_PANCRACUTE_AESI\n13.0\n9.452041\n27.29\n\n\n\nE_DM1_AESI\n38.0\n44.208164\n-16.34\n\n\n\nE_GOUT_AESI\n27.0\n25.344898\n6.13\n\n\n\nG_KIACUTE_AESI\n15.0\n14.710204\n1.93\n\n\n\nG_UTI_AESI\n23.0\n23.219387\n-0.95\n\n\n\nI_INFLUENZA_AESI\n16.0\n14.138776\n11.63\n\n\n\nM_FRACTURES_AESI\n60.0\n59.522449\n0.80\n\n\n\nM_OSTEOARTHRITIS_AESI\n28.0\n27.116327\n3.16\n\n\n\nN_STROKEHEMO_AESI\n10.0\n9.992857\n0.07\n\n\n\nSO_OTITISEXT_AESI\n36.0\n37.114285\n-3.10\n\n\n\nV_THROMBOSISARTERIALALGOR_AESI\n45.0\n45.730614\n-1.62\n\n\n\nV_VTEALGORITHM_AESI\n19.0\n19.263266\n-1.39\n\n\n\n\nThe script reproduce well most of the variables. However D_PANCRACUTE_AESI, C_MYOCARD_AESI and I_INFLUENZA_AESI are slightly underrepresented in the generated data, while E_DM1_AESI on the other hand is over represented.\nThe Mean Square Error is 5.63 which is good enough in this case.\nFinally I wantef to see the number of events each persons has:\n\n\n\nNumber of events\n0_real\n0_synthetic\ndiff_perc\n\n\n\n\n1.0\n639\n639.180612\n-0.03\n\n\n2.0\n52\n52.792857\n-1.52\n\n\n3.0\n28\n27.562245\n1.56\n\n\n\nThe script does a very good job in this case. I removed the impossible cases when the person has 0 events but even with that taken into account the performance does not drop much.\nThe Mean Square Error is only 0.28 which is very good."
  },
  {
    "objectID": "tentative_solutions.html",
    "href": "tentative_solutions.html",
    "title": "23  Tentative solutions",
    "section": "",
    "text": "At the beginning I analyzed what I found in the repository and compared it to the original MedGAN paper. I quickly noticed how the Generator was incomplete since the shortcut connections were missing.\nIn addition I tried:\n\nIncreasing Generator number of layers (size of the layer is fixed by Decoder and shortcut connections)\nIncreasing Discriminator number and size of layers. (best results by first increasing size and then decreasing it slowly)\nDecreasing learning rate and β and weight decay in Discriminator (and Generator indipendently)\nDefine a penalty to the Binary Cross Entropy in both Generator and Discriminator losses\nRetrain the AutoEncoder to increase the size of the Generator\nRegularization with L1\nSwitch activation function from Relu to Selu\nTrain the Generator N times each epoch while only once for the Discriminator. In the original paper was the other way around.\n\nWith some combination of hyperparameters I had the impression it was training better and more stable but I couldn’t get it to stop mode collapsing in the end."
  },
  {
    "objectID": "training_WGAN_GP.html",
    "href": "training_WGAN_GP.html",
    "title": "24  Training WGAN-GP",
    "section": "",
    "text": "After running the WGAN-GP on a limited number of epochs I could quickly tell that it was working.\nThis was probably possible by the changes previously done to the networks when I was testing GAN and WGAN.\nI then decided to halve the learning rate of both Generator and Discriminator and increase the number of epochs to 20000 to obtain the best possible result.\nFinally I was able to train the final model in around 6 hours on Colab. The final model was then saved on the repository as checkpoint using git.\nThis final model’s discriminator loss reached a state of bouncing around 0, so small enough, and consequently the trained Generator should be quite good, as explained before.\nThe generator loss was around -3700 but that does not influence the model performance at all so it is not a metric for it."
  },
  {
    "objectID": "WGAN.html",
    "href": "WGAN.html",
    "title": "25  WGAN",
    "section": "",
    "text": "I tried searching for an easier model to train and I found the WGAN.\nIt promises improve stability and less probability of diverging.\nThis is possible because this time we are not constraining the generator but only the discriminator. At the same time what we want to use from the WGAN in the end is the Generator to create synthetic data.\nBut if we are not defining a loss function how can we evaluate the Generator performance?\n\n\n\nIt’s easy!\n\n\nAnother very important advantage of WGAN over traditional GAN is that the discriminator loss function should be connected to the quality of the simulated data using that model and consequently easier to interpret. Lower loss values should translate in better generation of synthetic data.\nHowever, I quickly discovered how even with the vanilla WGAN I had problems in getting useful results because, in addition to the hyperparameters defined in the previous section, I needed to setup the clipping factor of the weights that enforces the Lipschitz constraint.\nI had the impression fine tuning that parameter wouldn’t be easy. At the same time the WGAN was a clear improvement over the simple GAN so I tried to find other researches/articles in how to setup correctly the clipping factor."
  },
  {
    "objectID": "WGAN_GP.html",
    "href": "WGAN_GP.html",
    "title": "26  WGAN-GP",
    "section": "",
    "text": "Searching for WGAN models tutorials lead me to the a derivative model called WGAN-GP from an example in the Keras documentation.\nThe original was easy to find here: (Gulrajani et al. 2017)\nHere the enforcing of the Lipschitz constraints is not done by clipping weights but in alternative way through a gradient penalty. This a softer version of the original constraint since we want the weights to converge to 1 and not simply clip them as before.\nIn the code from Keras documentation the generate the interpolated data for the gradient penalty by calculating the difference between generated and real data multiplied by a variable sampled from a uniform(0, 1) which is then added to the real data.\nIn the original paper however the first term is added to the generated image. I’m not sure why this is setup as such in the example but I changed it to reflect what is inside the manuscript.\nIn addition in the WGAN-GP they highlight how to not use batch normalization in the Discriminator so I removed all of them. In their stead I decide to use the Selu which normalize internally.\n\nFor the gradient penalty coefficient λ I decided to retain what is specified in the paper, so 10. Same treatment for learning rates, βs and number of times the Discriminator is trained each epoch.\n\n\n\n\nGulrajani, Ishaan, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. 2017. “Improved Training of Wasserstein GANs.” https://arxiv.org/abs/1704.00028."
  },
  {
    "objectID": "mode_collapse.html",
    "href": "mode_collapse.html",
    "title": "22  Mode Collapse",
    "section": "",
    "text": "Mode collapsing happens when the generator produce only a subset of the patters or modes found in the original distribution.\nIn our case all outputs are persons which have as event DEATH.\nTo understand why the model is mode collapsing I should describe in general how a GAN works\nThe idea is to use Generator to create a vector on the latent space generated by the Encoder and use the Decoder to convert those values which will then be compared, by the discriminator, to an original sample.\nThe issue is, many times, the Discriminator is much more powerful than the Generator: the Generator won’t have time to learn all the features before the Discriminator became too good and prevents the Generator to further improve its performance.\nObviously I didn’t want the GAN to mode collapse so I tried find solutions"
  }
]