{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlUafk1yg9v2xgmIH2lPrT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary packages and functions"
      ],
      "metadata": {
        "id": "ElPEP8AqxbTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Hl9i9wxA1WB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parameters"
      ],
      "metadata": {
        "id": "qTik8n2IPZPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GIT_USERNAME = \"dado330\"\n",
        "GIT_MAIL = \"messina.davide.statistician@gmail.com\"\n",
        "GIT_REPO = \"https://github.com/dado330/AE-WGAN-GP\"\n",
        "\n",
        "train_AE = False\n",
        "train_WGAN_GP = False\n",
        "train_from_partial = True\n",
        "\n",
        "batchsize = 20\n",
        "Z_DIM = 512\n",
        "random.seed(123)"
      ],
      "metadata": {
        "id": "x8PmxJ3mPZbM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git parameter to be run only once"
      ],
      "metadata": {
        "id": "PjHebxjn8E33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone $GIT_REPO"
      ],
      "metadata": {
        "id": "yU9JBz1h8FC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pull changes before running the script"
      ],
      "metadata": {
        "id": "VKglv2hhSaAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git pull origin main"
      ],
      "metadata": {
        "id": "5X1YaAvwSZtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git add -A\n",
        "git commit -m \"Created folder for generated data populated with still not ok data\"\n",
        "git push -u origin main"
      ],
      "metadata": {
        "id": "f3ErTOCofzEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU has been loaded correctly"
      ],
      "metadata": {
        "id": "OMivdUQGzGvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_AE or train_WGAN_GP:\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name != '/device:GPU:0':\n",
        "        raise SystemError('GPU device not found')\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "TBsBrGaEClo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of encoder: input is with dimension 18 and has four dense layers of increasing size 64, 128, 256, 512\n",
        "\n",
        "Internal layers have Selu as activation function while the output layer has the sigmoid function\n",
        "\n",
        "The model is fairly big with respect to the data so I used l1 as kernel regularizer to make it possible for the model to set some of the nodes to 0"
      ],
      "metadata": {
        "id": "9b9ai37MzO6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.Encoder_DIMS = [64, 128, 256, 512]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Encoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Encoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for s in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[s](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lIuFRwHXxZkf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of decoder: input is with dimension 512 and has four dense layers of decreasing size 256, 128, 64, 18\n",
        "\n",
        "Internal layers have Selu as activation function while the output layer has the sigmoid function. The sigmoid in this is necessary since each cell of our dataset is binary\n",
        "\n",
        "The model is fairly big with respect to the data so I used l1 as kernel regularizer to make it possible for the model to set some of the nodes to 0"
      ],
      "metadata": {
        "id": "LtY3jdijzpSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.Decoder_DIMS = [256, 128, 64, 18]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Decoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Decoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for j in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[j](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UAp9WeepzNOc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model for the autoencoder and create the checkpoint manager\n",
        "\n",
        "Define the optimizer as Adam with relative low learning rate and betas. Added weigth decay to improve regularization (similar to l2)\n",
        "\n",
        "Use bynary crossentropy as the main loss function"
      ],
      "metadata": {
        "id": "IeJq2nFhizYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "\n",
        "checkpoint_directory_ae = \"/content/AE-WGAN-GP/AE_checkpoints\"\n",
        "\n",
        "checkpoint_ae = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
        "manager_ae = tf.train.CheckpointManager(checkpoint_ae, directory=checkpoint_directory_ae, max_to_keep=5)\n",
        "\n",
        "ae_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                             beta_1=0.7, beta_2=0.99)\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n"
      ],
      "metadata": {
        "id": "ut6iDaaMi0Ls"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the autoencoder\n",
        "\n",
        "The loss function is not the vanilla binary crossentropy because otherwise the model was collapsing to all 0 or mode collapsing"
      ],
      "metadata": {
        "id": "sZ6w9p7H11vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_step(real):\n",
        "    with (tf.GradientTape() as disc_tape):\n",
        "        synthetic = decoder(encoder(real))\n",
        "\n",
        "        main_sparsity_penalty = - tf.math.log(tf.reduce_mean(synthetic + 1e-10)) + tf.math.log(\n",
        "            tf.reduce_mean(tf.cast(real, tf.float32) + 1e-10))\n",
        "        sparsity_penalty = - tf.math.log(1 - main_sparsity_penalty)\n",
        "        autoenc_loss = tf.cast(bce(synthetic, real), tf.float32) + sparsity_penalty\n",
        "        if np.random.uniform(size=1) > 0.999:\n",
        "            print(\"synthetic\")\n",
        "            print(tf.round(synthetic).numpy())\n",
        "            print(\"real\")\n",
        "            print(real.numpy())\n",
        "\n",
        "    gradients_of_autoencoder = disc_tape.gradient(autoenc_loss,\n",
        "                                                  encoder.trainable_variables + decoder.trainable_variables)\n",
        "    ae_optimizer.apply_gradients(zip(gradients_of_autoencoder,\n",
        "                                     encoder.trainable_variables + decoder.trainable_variables))\n",
        "    return autoenc_loss"
      ],
      "metadata": {
        "id": "XCzIjlMs11jN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom learning rate scheduler"
      ],
      "metadata": {
        "id": "-Dyi77pP0lpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_learning_rate(epoch):\n",
        "    if epoch > 500:\n",
        "        ae_optimizer.learning_rate = 1e-4\n",
        "    elif epoch > 50:\n",
        "        ae_optimizer.learning_rate = 3e-4\n",
        "    elif epoch > 3:\n",
        "        ae_optimizer.learning_rate = 6e-4"
      ],
      "metadata": {
        "id": "PnE47D_i0mF3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data, divide it in train and test datasets"
      ],
      "metadata": {
        "id": "Z-vZ9qICiWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)\n",
        "train_data, test_data = train_test_split(data, test_size=0.1)"
      ],
      "metadata": {
        "id": "GQgZEwsbiWjd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the autoencoder and save it"
      ],
      "metadata": {
        "id": "PkMtO0qNiRPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_AE:\n",
        "    epochs = 1000\n",
        "    steps = len(dataset_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        set_learning_rate(epoch)\n",
        "        aeloss = 0.0\n",
        "        for batch_sample in dataset_train:\n",
        "            aeloss += ae_step(batch_sample)\n",
        "        duration_epoch_ae = time.time() - start_time\n",
        "        format_str = 'epoch: %d, aeloss = %.3f (%.2f)'\n",
        "        print(format_str % (epoch, aeloss / steps / batchsize * 10000, duration_epoch_ae))\n",
        "\n",
        "    manager_ae.save()\n",
        "else:\n",
        "    checkpoint_ae.restore(manager_ae.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n"
      ],
      "metadata": {
        "id": "TKt6Yn6-iRFd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the loss of the autoencoder for both train and test data"
      ],
      "metadata": {
        "id": "QC4-tVGFridN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aeloss_train = ae_step(train_data)\n",
        "aeloss_test = ae_step(test_data)\n",
        "format_str = 'train_aeloss: %.3f, test_aeloss = %.3f'\n",
        "print(format_str % (aeloss_train / len(train_data) * 10000, aeloss_test / len(test_data) * 10000))"
      ],
      "metadata": {
        "id": "9trDPqSfriAW",
        "outputId": "c33e64eb-ea45-4f13-fc08-36dbe5f2c641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_aeloss: -0.000, test_aeloss = -0.039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of generator: input is with dimension 512 and has three dense layers of fixed size 512\n",
        "\n",
        "Size the layers is fixed beacuse it is needed for the passtrougth\n",
        "\n",
        "Internal layers have Selu as activation function while the output layer has the sigmoid function. The sigmoid in this is necessary since it should be the same of the autoencoder.\n",
        "\n",
        "The model is fairly big with respect to the data so I used l1 as kernel regularizer to make it possible for the model to set some of the nodes to 0\n",
        "\n",
        "Added batch normalization before every dense layer"
      ],
      "metadata": {
        "id": "2AAJmkBUz6tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.G_DIMS = [512, 512, 512]\n",
        "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.G_DIMS[1:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        for k in range(0, len(self.G_DIMS[:-2])):\n",
        "            x1 = self.dense_layers[k](self.batch_norm_layers[k](x, training=training))\n",
        "            x += tf.keras.layers.Add()([x1, x])\n",
        "        x2 = self.output_layer(self.batch_norm_layers[-1](x, training=training))\n",
        "        x += tf.keras.layers.Add()([x2, x])\n",
        "        return x"
      ],
      "metadata": {
        "id": "KOavwPS1z6c8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of discriminator: input is with dimension 512 and has five dense layers of decreasing size 256, 128, 64, 32, 1\n",
        "\n",
        "Internal layers have Selu as activation function while the output layer has the linear function. The linear is necessary for the WGAN.\n",
        "\n",
        "Removed batch normalization from discriminator since it is not suggested for WGAN"
      ],
      "metadata": {
        "id": "qsh3Fefw0M0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.D_DIMS = [256, 128, 64, 32]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu) for dim in self.D_DIMS]\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):\n",
        "        for h in range(len(self.D_DIMS)):\n",
        "            x = self.dense_layers[h](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CVU-gPqB0MRM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model for the encoder, decoder, generator and discriminator\n",
        "\n",
        "Create the checkpoint manager and restore the trained autoencoder. The decoder is not trainable now\n",
        "\n",
        "Define the optimizer as Adam with relative low learning rate and very low betas. Added weigth decay to improve regularization (similar to l2)"
      ],
      "metadata": {
        "id": "B-bkdtdEy6NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "checkpoint_directory = \"/content/AE-WGAN-GP/WGAN-GP_checkpoints\"\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder, generator=generator, discriminator=discriminator)\n",
        "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_directory, max_to_keep=5)\n",
        "\n",
        "checkpoint_ae.restore(manager_ae.latest_checkpoint).expect_partial().assert_consumed()\n",
        "decoder.trainable = False\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                    beta_1=0, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                        beta_1=0, beta_2=0.9)"
      ],
      "metadata": {
        "id": "MATkYYapyyBI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the gp (gradient penalty)"
      ],
      "metadata": {
        "id": "SwU45sK1024_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(batch_size, real_data, fake_data):\n",
        "    \"\"\"Calculates the gradient penalty.\n",
        "\n",
        "    This loss is calculated on an interpolated data row\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "    # Get the interpolated data row\n",
        "    real_data = tf.cast(real_data, tf.float32)\n",
        "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
        "    diff = real_data - fake_data\n",
        "    interpolated = fake_data + alpha * diff\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        # 1. Get the discriminator output for this interpolated data row.\n",
        "        pred = discriminator(interpolated, training=True)\n",
        "\n",
        "    # 2. Calculate the gradients w.r.t to this interpolated data row.\n",
        "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "    # 3. Calculate the norm of the gradients.\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
        "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "9tbmAjdE03NM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the discriminator/critic"
      ],
      "metadata": {
        "id": "dVLKXgMJ2Ae5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def d_step(real):\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    gp_weight = 10\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "        real_output = discriminator(real)\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        disc_cost = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "        gp = gradient_penalty(batchsize, real, synthetic)\n",
        "\n",
        "        disc_loss = disc_cost + gp * gp_weight\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    return disc_loss"
      ],
      "metadata": {
        "id": "O23pXbqi2AWH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the generator"
      ],
      "metadata": {
        "id": "Fb_5HtLK2RdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g_step():\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        synthetic = decoder(generator(z, training=True))\n",
        "\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss,\n",
        "                                               generator.trainable_variables + decoder.non_trainable_variables)\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables + decoder.non_trainable_variables))\n",
        "    return gen_loss"
      ],
      "metadata": {
        "id": "7iYYep8e2RSU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data again in case autoencoder was run in a previous session and to retain only train data (don't need test data anymore)"
      ],
      "metadata": {
        "id": "r9hVubUWx-vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "aRipy0K-foUO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the generator and discriminator/critic"
      ],
      "metadata": {
        "id": "oEv_Nyej1O-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_WGAN_GP:\n",
        "    dloss_list = list()\n",
        "    gloss_list = list()\n",
        "    steps = len(dataset_train)\n",
        "\n",
        "    if train_from_partial:\n",
        "        checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n",
        "        with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "            dloss_list = pickle.load(f)\n",
        "        with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "            gloss_list = pickle.load(f)\n",
        "\n",
        "    for epoch in range(10000):\n",
        "        start_time = time.time()\n",
        "        dloss = 0.0\n",
        "        gloss = 0.0\n",
        "        d_iter = 5\n",
        "        for batch_sample in dataset_train:\n",
        "            for _ in range(d_iter):\n",
        "                dloss += d_step(batch_sample)\n",
        "            gloss += g_step()\n",
        "\n",
        "        dloss_list.append(round(dloss.numpy() / (steps * d_iter * batchsize) * 10000, 1))\n",
        "        gloss_list.append(round(gloss.numpy() / steps / batchsize * 10000, 1))\n",
        "\n",
        "        duration_epoch = time.time() - start_time\n",
        "        format_str = 'epoch: %d, dloss = %.1f, gloss = %.1f, total_loss = %.1f (%.2f)'\n",
        "        print(format_str % (epoch, dloss / (steps * d_iter * batchsize) * 10000, gloss / steps / batchsize * 10000,\n",
        "         (dloss + gloss) / steps / batchsize * 1000, duration_epoch))\n",
        "\n",
        "    manager.save()\n",
        "    with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'wb') as f:\n",
        "        pickle.dump(dloss_list, f)\n",
        "    with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'wb') as f:\n",
        "        pickle.dump(gloss_list, f)"
      ],
      "metadata": {
        "id": "Mv7KKLVt1PTU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new set of data"
      ],
      "metadata": {
        "id": "7-7TRZCW4pwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "discriminator = Discriminator()\n",
        "decoder.trainable = False\n",
        "checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_consumed()\n",
        "\n",
        "def g_step_generator():\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "    return synthetic\n",
        "\n",
        "synthetic = g_step_generator()\n",
        "syn = discriminator(synthetic)\n",
        "print(tf.round(synthetic))\n",
        "\n",
        "np.save(\"/content/AE-WGAN-GP/Generated_data/output_file.npy\", train_data)"
      ],
      "metadata": {
        "id": "sfv10-s0Avq3",
        "outputId": "5773f062-b8d2-4691-b94d-2e4e8656b8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Unresolved object in checkpoint (root).decoder.output_layer.kernel: attributes {\n  name: \"VARIABLE_VALUE\"\n  full_name: \"decoder/dense_14/kernel\"\n  checkpoint_key: \"decoder/output_layer/kernel/.ATTRIBUTES/VARIABLE_VALUE\"\n}\nhas_checkpoint_values {\n  value: true\n}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-43e516fa6c8f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_consumed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mg_step_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36massert_consumed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0mtrackable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_by_proto_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtrackable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         raise AssertionError(\n\u001b[0m\u001b[1;32m    823\u001b[0m             \u001b[0;34m\"Unresolved object in checkpoint \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             f\"{pretty_printer.node_names[node_id]}: {node}\")\n",
            "\u001b[0;31mAssertionError\u001b[0m: Unresolved object in checkpoint (root).decoder.output_layer.kernel: attributes {\n  name: \"VARIABLE_VALUE\"\n  full_name: \"decoder/dense_14/kernel\"\n  checkpoint_key: \"decoder/output_layer/kernel/.ATTRIBUTES/VARIABLE_VALUE\"\n}\nhas_checkpoint_values {\n  value: true\n}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "62XNwwa2O4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the discriminator loss"
      ],
      "metadata": {
        "id": "X4lRn9gD8HT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_directory = \"/content/gdrive/My Drive/Colab_Notebooks/training_checkpoints_medgan\"\n",
        "with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "  dloss_list = pickle.load(f)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "plt.title('Discriminator/Critic Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "ax.set_ylim(-200, 200)\n",
        "plt.title('Discriminator/Critic Loss (Zoom in)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lEHXrzpP9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the generator loss"
      ],
      "metadata": {
        "id": "017l0Bcs8EmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "  gloss_list = pickle.load(f)\n",
        "plt.plot(gloss_list)\n",
        "plt.title('Generator Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('g-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hfl-XuP6xPRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}