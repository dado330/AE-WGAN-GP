{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF41+cMa2plMgq9S6IU8cd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary packages and functions"
      ],
      "metadata": {
        "id": "ElPEP8AqxbTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_Hl9i9wxA1WB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parameters"
      ],
      "metadata": {
        "id": "qTik8n2IPZPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GIT_USERNAME = \"dado330\"\n",
        "GIT_MAIL = \"messina.davide.statistician@gmail.com\"\n",
        "GIT_REPO = \"https://github.com/dado330/AE-WGAN-GP\"\n",
        "\n",
        "train_AE = False\n",
        "train_WGAN_GP = False\n",
        "# decoder.trainable = False\n",
        "train_from_partial = True\n",
        "\n",
        "batchsize = 20\n",
        "Z_DIM = 512\n",
        "random.seed(123)"
      ],
      "metadata": {
        "id": "x8PmxJ3mPZbM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git parameter to be run only once"
      ],
      "metadata": {
        "id": "PjHebxjn8E33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone $GIT_REPO\n",
        "!git config --global user.email $GIT_MAIL\n",
        "!git config --global user.name $GIT_USERNAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU9JBz1h8FC-",
        "outputId": "78c4824c-d2f8-4222-c463-e36a86eb35e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AE-WGAN-GP' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pull changes before running the script"
      ],
      "metadata": {
        "id": "VKglv2hhSaAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X1YaAvwSZtj",
        "outputId": "2182a7f6-c0d3-4c31-af70-d59abb33c744"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From https://github.com/dado330/AE-WGAN-GP\n",
            " * branch            main       -> FETCH_HEAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git add -A\n",
        "git commit -m \"Created folder for generated data populated with still not ok data\"\n",
        "git push -u origin main"
      ],
      "metadata": {
        "id": "f3ErTOCofzEC",
        "outputId": "0273c53a-3f41-4004-bfa7-bd270d8accb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://github.com/dado330/AE-WGAN-GP.git\n",
            "   0834415..274cc33  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU has been loaded correctly"
      ],
      "metadata": {
        "id": "OMivdUQGzGvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "TBsBrGaEClo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04630c80-19d1-44ea-e7d5-3dd188553be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of encoder: input is with dimension 18 and has four dense layers of increasing size 64, 128, 256, 512"
      ],
      "metadata": {
        "id": "9b9ai37MzO6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.Encoder_DIMS = [64, 128, 256, 512]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Encoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Encoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for s in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[s](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lIuFRwHXxZkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of decoder: input is with dimension 512 and has four dense layers of decreasing size 256, 128, 64, 18"
      ],
      "metadata": {
        "id": "LtY3jdijzpSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.Decoder_DIMS = [256, 128, 64, 18]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Decoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Decoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for j in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[j](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UAp9WeepzNOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model for the autoencoder, define the optimizer, create the checkpoint manager and main loss function"
      ],
      "metadata": {
        "id": "IeJq2nFhizYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                             beta_1=0.7, beta_2=0.99)\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "\n",
        "checkpoint_directory_ae = \"/content/AE-WGAN-GP/AE_checkpoints\"\n",
        "\n",
        "checkpoint_ae = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
        "manager_ae = tf.train.CheckpointManager(checkpoint_ae, directory=checkpoint_directory_ae, max_to_keep=5)\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n"
      ],
      "metadata": {
        "id": "ut6iDaaMi0Ls"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the autoencoder"
      ],
      "metadata": {
        "id": "sZ6w9p7H11vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_step(real):\n",
        "    with (tf.GradientTape() as disc_tape):\n",
        "        synthetic = decoder(encoder(real))\n",
        "\n",
        "        main_sparsity_penalty = - tf.math.log(tf.reduce_mean(synthetic + 1e-10)) + tf.math.log(\n",
        "            tf.reduce_mean(tf.cast(real, tf.float32) + 1e-10))\n",
        "        sparsity_penalty = - tf.math.log(1 - main_sparsity_penalty)\n",
        "        autoenc_loss = tf.cast(bce(synthetic, real), tf.float32) + sparsity_penalty\n",
        "        if np.random.uniform(size=1) > 0.999:\n",
        "            print(\"synthetic\")\n",
        "            print(tf.round(synthetic).numpy())\n",
        "            print(\"real\")\n",
        "            print(real.numpy())\n",
        "\n",
        "    gradients_of_autoencoder = disc_tape.gradient(autoenc_loss,\n",
        "                                                  encoder.trainable_variables + decoder.trainable_variables)\n",
        "    ae_optimizer.apply_gradients(zip(gradients_of_autoencoder,\n",
        "                                     encoder.trainable_variables + decoder.trainable_variables))\n",
        "    return autoenc_loss"
      ],
      "metadata": {
        "id": "XCzIjlMs11jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom learning rate scheduler"
      ],
      "metadata": {
        "id": "-Dyi77pP0lpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_learning_rate(epoch):\n",
        "    if epoch > 500:\n",
        "        ae_optimizer.learning_rate = 1e-4\n",
        "    elif epoch > 50:\n",
        "        ae_optimizer.learning_rate = 3e-4\n",
        "    elif epoch > 3:\n",
        "        ae_optimizer.learning_rate = 6e-4"
      ],
      "metadata": {
        "id": "PnE47D_i0mF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data, divide it in train and test datasets"
      ],
      "metadata": {
        "id": "Z-vZ9qICiWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)\n",
        "train_data, test_data = train_test_split(data, test_size=0.1)"
      ],
      "metadata": {
        "id": "GQgZEwsbiWjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the autoencoder"
      ],
      "metadata": {
        "id": "PkMtO0qNiRPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_AE:\n",
        "    epochs = 1000\n",
        "    steps = len(dataset_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        set_learning_rate(epoch)\n",
        "        aeloss = 0.0\n",
        "        for batch_sample in dataset_train:\n",
        "            aeloss += ae_step(batch_sample)\n",
        "        duration_epoch_ae = time.time() - start_time\n",
        "        format_str = 'epoch: %d, aeloss = %.3f (%.2f)'\n",
        "        print(format_str % (epoch, aeloss / steps / batchsize * 10000, duration_epoch_ae))\n",
        "\n",
        "    manager_ae.save()\n",
        "else:\n",
        "    checkpoint_ae.restore(manager_ae.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n"
      ],
      "metadata": {
        "id": "TKt6Yn6-iRFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ger the loss of the autoencoder for both train and test data"
      ],
      "metadata": {
        "id": "QC4-tVGFridN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aeloss_train = ae_step(train_data)\n",
        "aeloss_test = ae_step(test_data)\n",
        "format_str = 'train_aeloss: %.3f, test_aeloss = %.3f'\n",
        "print(format_str % (aeloss_train / len(train_data) * 10000, aeloss_test / len(test_data) * 10000))"
      ],
      "metadata": {
        "id": "9trDPqSfriAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of generator: input is with dimension 512 and has three dense layers of fixed size 512"
      ],
      "metadata": {
        "id": "2AAJmkBUz6tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.G_DIMS = [512, 512, 512]\n",
        "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.G_DIMS[1:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        for k in range(0, len(self.G_DIMS[:-2])):\n",
        "            x1 = self.dense_layers[k](self.batch_norm_layers[k](x, training=training))\n",
        "            x += tf.keras.layers.Add()([x1, x])\n",
        "        x2 = self.output_layer(self.batch_norm_layers[-1](x, training=training))\n",
        "        x += tf.keras.layers.Add()([x2, x])\n",
        "        return x"
      ],
      "metadata": {
        "id": "KOavwPS1z6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of discriminator: input is with dimension 512 and has five dense layers of decreasing size 256, 128, 64, 32, 1"
      ],
      "metadata": {
        "id": "qsh3Fefw0M0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.D_DIMS = [256, 128, 64, 32]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu) for dim in self.D_DIMS]\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):\n",
        "        for h in range(len(self.D_DIMS)):\n",
        "            x = self.dense_layers[h](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CVU-gPqB0MRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model for the encoder, decoder, generator and discriminator\n",
        "\n",
        "Define the optimizers and create the checkpoint manager\n",
        "\n",
        "Restore the trained autoencoder"
      ],
      "metadata": {
        "id": "B-bkdtdEy6NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                    beta_1=0, beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                        beta_1=0, beta_2=0.9)\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "checkpoint_directory = \"/content/AE-WGAN-GP/WGAN-GP_checkpoints\"\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder, generator=generator, discriminator=discriminator)\n",
        "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_directory, max_to_keep=5)\n",
        "\n",
        "checkpoint_ae.restore(manager_ae.latest_checkpoint).expect_partial().assert_existing_objects_matched()"
      ],
      "metadata": {
        "id": "MATkYYapyyBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the gp (gradient penalty)"
      ],
      "metadata": {
        "id": "SwU45sK1024_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(batch_size, real_data, fake_data):\n",
        "    \"\"\"Calculates the gradient penalty.\n",
        "\n",
        "    This loss is calculated on an interpolated data row\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "    # Get the interpolated data row\n",
        "    real_data = tf.cast(real_data, tf.float32)\n",
        "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
        "    diff = real_data - fake_data\n",
        "    interpolated = fake_data + alpha * diff\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        # 1. Get the discriminator output for this interpolated data row.\n",
        "        pred = discriminator(interpolated, training=True)\n",
        "\n",
        "    # 2. Calculate the gradients w.r.t to this interpolated data row.\n",
        "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "    # 3. Calculate the norm of the gradients.\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
        "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "9tbmAjdE03NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the discriminator/critic"
      ],
      "metadata": {
        "id": "dVLKXgMJ2Ae5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def d_step(real):\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    gp_weight = 10\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "        real_output = discriminator(real)\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        disc_cost = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "        gp = gradient_penalty(batchsize, real, synthetic)\n",
        "\n",
        "        disc_loss = disc_cost + gp * gp_weight\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    return disc_loss"
      ],
      "metadata": {
        "id": "O23pXbqi2AWH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the generator"
      ],
      "metadata": {
        "id": "Fb_5HtLK2RdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g_step():\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        synthetic = decoder(generator(z, training=True))\n",
        "\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss,\n",
        "                                               generator.trainable_variables + decoder.non_trainable_variables)\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables + decoder.non_trainable_variables))\n",
        "    return gen_loss"
      ],
      "metadata": {
        "id": "7iYYep8e2RSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data again in case autoencoder was run in a previous session and to retain only train data (don't need test data anymore)"
      ],
      "metadata": {
        "id": "r9hVubUWx-vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "aRipy0K-foUO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the generator and discriminator/critic"
      ],
      "metadata": {
        "id": "oEv_Nyej1O-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_WGAN_GP:\n",
        "dloss_list = list()\n",
        "gloss_list = list()\n",
        "steps = len(dataset_train)\n",
        "train_WGAN_GP\n",
        "\n",
        "\n",
        "if train_from_partial:\n",
        "    checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n",
        "    with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "        dloss_list = pickle.load(f)\n",
        "    with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "        gloss_list = pickle.load(f)\n",
        "\n",
        "for epoch in range(10000):\n",
        "        start_time = time.time()\n",
        "        dloss = 0.0\n",
        "        gloss = 0.0\n",
        "        d_iter = 5\n",
        "        for batch_sample in dataset_train:\n",
        "            for _ in range(d_iter):\n",
        "                dloss += d_step(batch_sample)\n",
        "            gloss += g_step()\n",
        "\n",
        "        dloss_list.append(round(dloss.numpy() / (steps * d_iter * batchsize) * 10000, 1))\n",
        "        gloss_list.append(round(gloss.numpy() / steps / batchsize * 10000, 1))\n",
        "\n",
        "        duration_epoch = time.time() - start_time\n",
        "        format_str = 'epoch: %d, dloss = %.1f, gloss = %.1f, total_loss = %.1f (%.2f)'\n",
        "        print(format_str % (epoch, dloss / (steps * d_iter * batchsize) * 10000, gloss / steps / batchsize * 10000,\n",
        "         (dloss + gloss) / steps / batchsize * 1000, duration_epoch))\n",
        "\n",
        "manager.save()\n",
        "with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'wb') as f:\n",
        "    pickle.dump(dloss_list, f)\n",
        "with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'wb') as f:\n",
        "    pickle.dump(gloss_list, f)"
      ],
      "metadata": {
        "id": "Mv7KKLVt1PTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new set of data"
      ],
      "metadata": {
        "id": "7-7TRZCW4pwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "discriminator = Discriminator()\n",
        "checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n",
        "\n",
        "\n",
        "def gen():\n",
        "\n",
        "    def g_step_generator():\n",
        "        z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "        synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "        return synthetic\n",
        "\n",
        "    synthetic = g_step_generator()\n",
        "    syn = discriminator(synthetic)\n",
        "    print(tf.round(synthetic))\n",
        "    # format_str = 'percentage of generated data classified as true: %d%%'\n",
        "    # print(format_str % (np.sum(tf.round(syn) == 0) / batchsize * 100))\n",
        "    # y = tf.gather(tf.round(synthetic), tf.where(tf.squeeze(tf.round(syn) == 0)))\n",
        "    # print(tf.squeeze(y))\n",
        "\n",
        "    np.save(\"/content/AE-WGAN-GP/Generated_data/output_file.npy\", train_data)"
      ],
      "metadata": {
        "id": "sfv10-s0Avq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "62XNwwa2O4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the discriminator loss"
      ],
      "metadata": {
        "id": "X4lRn9gD8HT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_directory = \"/content/gdrive/My Drive/Colab_Notebooks/training_checkpoints_medgan\"\n",
        "with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "  dloss_list = pickle.load(f)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "plt.title('Discriminator/Critic Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "ax.set_ylim(-200, 200)\n",
        "plt.title('Discriminator/Critic Loss (Zoom in)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lEHXrzpP9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the generator loss"
      ],
      "metadata": {
        "id": "017l0Bcs8EmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "  gloss_list = pickle.load(f)\n",
        "plt.plot(gloss_list)\n",
        "plt.title('Generator Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('g-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hfl-XuP6xPRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}