{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVFVPUjRRKSDpFcuBthXwN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary packages and functions"
      ],
      "metadata": {
        "id": "ElPEP8AqxbTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Hl9i9wxA1WB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parameters"
      ],
      "metadata": {
        "id": "qTik8n2IPZPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_directory = \"/content/AE-WGAN-GP/WGAN-GP_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
        "checkpoint_directory_ae = \"/content/AE-WGAN-GP/AE_checkpoints\"\n",
        "checkpoint_prefix_ae = os.path.join(checkpoint_directory_ae, \"ckpt\")\n",
        "\n",
        "GIT_USERNAME = \"dado330\"\n",
        "GIT_MAIL = \"messina.davide.statistician@gmail.com\"\n",
        "GIT_REPO = \"https://github.com/dado330/AE-WGAN-GP\""
      ],
      "metadata": {
        "id": "x8PmxJ3mPZbM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git parameter to be run only once"
      ],
      "metadata": {
        "id": "PjHebxjn8E33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone $GIT_REPO\n",
        "!git config --global user.email $GIT_MAIL\n",
        "!git config --global user.name $GIT_USERNAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU9JBz1h8FC-",
        "outputId": "28daf145-d8ac-4d9f-82f4-1059428ad7a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AE-WGAN-GP'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 23 (delta 5), reused 19 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (23/23), 9.30 MiB | 27.30 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pull changes before running the script"
      ],
      "metadata": {
        "id": "VKglv2hhSaAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X1YaAvwSZtj",
        "outputId": "c683df2a-9419-40f2-cafc-aef6f732fb60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From https://github.com/dado330/AE-WGAN-GP\n",
            " * branch            main       -> FETCH_HEAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/AE-WGAN-GP\n",
        "git add -A\n",
        "git commit -m \"first commit\"\n",
        "git push -u origin main"
      ],
      "metadata": {
        "id": "f3ErTOCofzEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU has been loaded correctly"
      ],
      "metadata": {
        "id": "OMivdUQGzGvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "TBsBrGaEClo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04630c80-19d1-44ea-e7d5-3dd188553be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of encoder: input is with dimension 18 and has four dense layers of increasing size 64, 128, 256, 512"
      ],
      "metadata": {
        "id": "9b9ai37MzO6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.Encoder_DIMS = [64, 128, 256, 512]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Encoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Encoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for s in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[s](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lIuFRwHXxZkf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of decoder: input is with dimension 512 and has four dense layers of decreasing size 256, 128, 64, 18"
      ],
      "metadata": {
        "id": "LtY3jdijzpSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.Decoder_DIMS = [256, 128, 64, 18]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.Decoder_DIMS[:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.Decoder_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        for j in range(len(self.dense_layers)):\n",
        "            x = self.dense_layers[j](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UAp9WeepzNOc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model for the autoencoder and define the optimizer"
      ],
      "metadata": {
        "id": "IeJq2nFhizYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                             beta_1=0.7, beta_2=0.99)\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()"
      ],
      "metadata": {
        "id": "ut6iDaaMi0Ls"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the autoencoder"
      ],
      "metadata": {
        "id": "sZ6w9p7H11vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ae_step(real):\n",
        "    with (tf.GradientTape() as disc_tape):\n",
        "        synthetic = decoder(encoder(real))\n",
        "\n",
        "        main_sparsity_penalty = - tf.math.log(tf.reduce_mean(synthetic + 1e-10)) + tf.math.log(\n",
        "            tf.reduce_mean(tf.cast(real, tf.float32) + 1e-10))\n",
        "        sparsity_penalty = - tf.math.log(1 - main_sparsity_penalty)\n",
        "        autoenc_loss = tf.cast(bce(synthetic, real), tf.float32) + sparsity_penalty\n",
        "        if np.random.uniform(size=1) > 0.999:\n",
        "            print(\"synthetic\")\n",
        "            print(tf.round(synthetic).numpy())\n",
        "            print(\"real\")\n",
        "            print(real.numpy())\n",
        "\n",
        "    gradients_of_autoencoder = disc_tape.gradient(autoenc_loss,\n",
        "                                                  encoder.trainable_variables + decoder.trainable_variables)\n",
        "    ae_optimizer.apply_gradients(zip(gradients_of_autoencoder,\n",
        "                                     encoder.trainable_variables + decoder.trainable_variables))\n",
        "    return autoenc_loss"
      ],
      "metadata": {
        "id": "XCzIjlMs11jN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom learning rate scheduler"
      ],
      "metadata": {
        "id": "-Dyi77pP0lpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_learning_rate(epoch):\n",
        "    if epoch > 500:\n",
        "        ae_optimizer.learning_rate = 1e-4\n",
        "    elif epoch > 50:\n",
        "        ae_optimizer.learning_rate = 3e-4\n",
        "    elif epoch > 3:\n",
        "        ae_optimizer.learning_rate = 6e-4"
      ],
      "metadata": {
        "id": "PnE47D_i0mF3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data, divide it in train and test datasets"
      ],
      "metadata": {
        "id": "Z-vZ9qICiWSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)\n",
        "train_data, test_data = train_test_split(data, test_size=0.1)"
      ],
      "metadata": {
        "id": "GQgZEwsbiWjd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the autoencoder"
      ],
      "metadata": {
        "id": "PkMtO0qNiRPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "steps = len(train_data)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    set_learning_rate(epoch)\n",
        "    start_time = time.time()\n",
        "    aeloss = 0.0\n",
        "    for batch_sample in dataset_train:\n",
        "        aeloss += ae_step(batch_sample)\n",
        "    duration_epoch_ae = time.time() - start_time\n",
        "    format_str = 'epoch: %d, aeloss = %.3f (%.2f)'\n",
        "    print(format_str % (epoch, aeloss / steps * 1000, duration_epoch_ae))\n",
        "    if (aeloss.numpy() < 0.001) & epoch == (epochs - 1):\n",
        "        checkpoint_ae.save(file_prefix=checkpoint_prefix_ae)\n",
        "\n",
        "checkpoint_ae.save(file_prefix=checkpoint_directory_ae_output)"
      ],
      "metadata": {
        "id": "TKt6Yn6-iRFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of generator: input is with dimension 512 and has three dense layers of fixed size 512"
      ],
      "metadata": {
        "id": "2AAJmkBUz6tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.G_DIMS = [512, 512, 512]\n",
        "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu, kernel_regularizer='l1')\n",
        "                             for dim in self.G_DIMS[1:-1]]\n",
        "        self.output_layer = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        for k in range(0, len(self.G_DIMS[:-2])):\n",
        "            x1 = self.dense_layers[k](self.batch_norm_layers[k](x, training=training))\n",
        "            x += tf.keras.layers.Add()([x1, x])\n",
        "        x2 = self.output_layer(self.batch_norm_layers[-1](x, training=training))\n",
        "        x += tf.keras.layers.Add()([x2, x])\n",
        "        return x"
      ],
      "metadata": {
        "id": "KOavwPS1z6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of discriminator: input is with dimension 512 and has five dense layers of decreasing size 256, 128, 64, 32, 1"
      ],
      "metadata": {
        "id": "qsh3Fefw0M0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.D_DIMS = [256, 128, 64, 32]\n",
        "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.selu) for dim in self.D_DIMS]\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):\n",
        "        for h in range(len(self.D_DIMS)):\n",
        "            x = self.dense_layers[h](x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CVU-gPqB0MRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the gp (gradient penalty)"
      ],
      "metadata": {
        "id": "SwU45sK1024_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(batch_size, real_data, fake_data):\n",
        "    \"\"\"Calculates the gradient penalty.\n",
        "\n",
        "    This loss is calculated on an interpolated image\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "    # Get the interpolated image\n",
        "    real_data = tf.cast(real_data, tf.float32)\n",
        "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
        "    diff = real_data - fake_data\n",
        "    interpolated = fake_data + alpha * diff\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        # 1. Get the discriminator output for this interpolated image.\n",
        "        pred = discriminator(interpolated, training=True)\n",
        "\n",
        "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "    # 3. Calculate the norm of the gradients.\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
        "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "    return gp"
      ],
      "metadata": {
        "id": "9tbmAjdE03NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the discriminator/critic"
      ],
      "metadata": {
        "id": "dVLKXgMJ2Ae5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def d_step(real):\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    gp_weight = 10\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "        real_output = discriminator(real)\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        disc_cost = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "        gp = gradient_penalty(batchsize, real, synthetic)\n",
        "\n",
        "        disc_loss = disc_cost + gp * gp_weight\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    return disc_loss"
      ],
      "metadata": {
        "id": "O23pXbqi2AWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training step for the generator"
      ],
      "metadata": {
        "id": "Fb_5HtLK2RdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g_step():\n",
        "    z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        synthetic = decoder(generator(z, training=True))\n",
        "\n",
        "        fake_output = discriminator(synthetic)\n",
        "\n",
        "        gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss,\n",
        "                                               generator.trainable_variables + decoder.non_trainable_variables)\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables + decoder.non_trainable_variables))\n",
        "    return gen_loss"
      ],
      "metadata": {
        "id": "7iYYep8e2RSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\", allow_pickle=True)\n",
        "train_data, test_data = train_test_split(data, test_size=0.1)\n",
        "steps = len(dataset_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "aRipy0K-foUO",
        "outputId": "1bfe0447-765d-4e06-f182-3afa16550b02"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5f3bef1f2ba5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/AE-WGAN-GP/Input_data/D3_events_ALL_OUTCOMES_ML.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                    beta_1=0, beta_2=0.9)\n",
        "    discriminator_optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0004,\n",
        "                                                        beta_1=0, beta_2=0.9)\n",
        "\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "    encoder = Encoder()\n",
        "    decoder = Decoder()\n",
        "\n",
        "    dloss_list = list()\n",
        "    gloss_list = list()\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "    checkpoint_ae = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder, generator=generator, discriminator=discriminator)\n",
        "    manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_directory, max_to_keep=5)\n",
        "\n",
        "    # def reinitialize(model):\n",
        "    #     for l in model.layers:\n",
        "    #         if hasattr(l, \"kernel_initializer\"):\n",
        "    #             l.kernel.assign(l.kernel_initializer(tf.shape(l.kernel)))\n",
        "    #         if hasattr(l, \"bias_initializer\"):\n",
        "    #             l.bias.assign(l.bias_initializer(tf.shape(l.bias)))\n",
        "    #         if hasattr(l, \"recurrent_initializer\"):\n",
        "    #             l.recurrent_kernel.assign(l.recurrent_initializer(tf.shape(l.recurrent_kernel)))\n",
        "\n",
        "    # epochs = 1000\n",
        "    # for epoch in range(epochs):\n",
        "    #     set_learning_rate(epoch)\n",
        "    #     start_time = time.time()\n",
        "    #     aeloss = 0.0\n",
        "    #     for batch_sample in dataset_train:\n",
        "    #         aeloss += ae_step(batch_sample)\n",
        "    #     duration_epoch_ae = time.time() - start_time\n",
        "    #     format_str = 'epoch: %d, aeloss = %.3f (%.2f)'\n",
        "    #     print(format_str % (epoch, aeloss / steps * 1000, duration_epoch_ae))\n",
        "    #     if (aeloss.numpy() < 0.001) & epoch == (epochs - 1):\n",
        "    #         checkpoint_ae.save(file_prefix=checkpoint_prefix_ae)\n",
        "\n",
        "    # checkpoint_ae.save(file_prefix=checkpoint_directory_ae_output)\n",
        "\n",
        "    # start_time = time.time()\n",
        "    # aeloss = ae_step(test_data)\n",
        "    # duration_epoch_ae = time.time() - start_time\n",
        "    # format_str = 'epoch: %d, test_aeloss = %.3f (%.2f)'\n",
        "    # print(format_str % (epoch, aeloss * 1000, duration_epoch_ae))\n",
        "\n",
        "    checkpoint_ae.restore(checkpoint_prefix_ae + '-1').expect_partial()\n",
        "    decoder.trainable = False\n",
        "    restore_results = False\n",
        "\n",
        "    if restore_results:\n",
        "        checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n",
        "        # checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_consumed()\n",
        "        with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "            dloss_list = pickle.load(f)\n",
        "        with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "            gloss_list = pickle.load(f)\n",
        "\n",
        "    for epoch in range(10000):\n",
        "        start_time = time.time()\n",
        "        dloss = 0.0\n",
        "        gloss = 0.0\n",
        "        step = 0.0\n",
        "        d_iter = 5\n",
        "        for batch_sample in dataset_train:\n",
        "            for _ in range(d_iter):\n",
        "                dloss += d_step(batch_sample)\n",
        "            gloss += g_step()\n",
        "            step += 1\n",
        "\n",
        "        dloss_list.append(round(dloss.numpy() / (step * d_iter) * 1000, 1))\n",
        "        gloss_list.append(round(gloss.numpy() / step * 1000, 1))\n",
        "\n",
        "        duration_epoch = time.time() - start_time\n",
        "        format_str = 'epoch: %d, dloss = %.1f, gloss = %.1f, total_loss = %.1f (%.2f)'\n",
        "        print(format_str % (epoch, dloss / (step * d_iter) * 1000, gloss / step * 1000,\n",
        "         (dloss + gloss) / step * 1000, duration_epoch))\n",
        "\n",
        "    manager.save()\n",
        "    with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'wb') as f:  # open a text file\n",
        "        pickle.dump(dloss_list, f)  # serialize the list\n",
        "    with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'wb') as f:  # open a text file\n",
        "        pickle.dump(gloss_list, f)  # serialize the list\n",
        "\n",
        "\n",
        "def gen():\n",
        "    checkpoint_directory = \"/content/gdrive/My Drive/Colab_Notebooks/training_checkpoints_medgan\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
        "    generator = Generator()\n",
        "    encoder = Encoder()\n",
        "    decoder = Decoder()\n",
        "    discriminator = Discriminator()\n",
        "    checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder, generator=generator, discriminator=discriminator)\n",
        "\n",
        "    manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_directory, max_to_keep=5)\n",
        "    checkpoint.restore(manager.latest_checkpoint).expect_partial().assert_existing_objects_matched()\n",
        "\n",
        "    @tf.function\n",
        "    def g_step():\n",
        "        z = tf.random.normal(shape=[batchsize, Z_DIM])\n",
        "        synthetic = decoder(generator(z, training=False))\n",
        "\n",
        "        return synthetic\n",
        "\n",
        "    synthetic = g_step()\n",
        "    syn = discriminator(synthetic)\n",
        "    print(tf.round(synthetic))\n",
        "    format_str = 'percentage of generated data classified as true: %d%%'\n",
        "    print(format_str % (np.sum(tf.round(syn) == 0) / batchsize * 100))\n",
        "    y = tf.gather(tf.round(synthetic), tf.where(tf.squeeze(tf.round(syn) == 0)))\n",
        "    print(tf.squeeze(y))\n",
        "\n",
        "    # np.save('syn/medgan', synthetic.numpy())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batchsize = 20\n",
        "    Z_DIM = 512\n",
        "    random.seed(123)\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('gpu', type=str)\n",
        "    # args = parser.parse_args()\n",
        "    # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    # os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    # gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "    # for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
        "    for i in range(1):\n",
        "        # train()\n",
        "        gen()\n",
        "    # gen(args.n, args.epoch)\n"
      ],
      "metadata": {
        "id": "sfv10-s0Avq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23836de1-8ff4-42cf-b6d4-1e8a5260e43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(20, 18), dtype=float32)\n",
            "percentage of generated data classified as true: 0%\n",
            "tf.Tensor([], shape=(0, 18), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/training_checkpoints_ae_4 /content/gdrive/MyDrive/Colab_Notebooks -r"
      ],
      "metadata": {
        "id": "Sh78kVVs-JlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "encoder = Encoder()\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "62XNwwa2O4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_directory = \"/content/gdrive/My Drive/Colab_Notebooks/training_checkpoints_medgan\"\n",
        "with open(os.path.join(checkpoint_directory, \"dloss.pkl\"), 'rb') as f:\n",
        "  dloss_list = pickle.load(f)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "plt.title('Discriminator/Critic Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(dloss_list)\n",
        "ax.set_ylim(-200, 200)\n",
        "plt.title('Discriminator/Critic Loss (Zoom in)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('d-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lEHXrzpP9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(checkpoint_directory, \"gloss.pkl\"), 'rb') as f:\n",
        "  gloss_list = pickle.load(f)\n",
        "plt.plot(gloss_list)\n",
        "plt.title('Generator Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend('g-loss', loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hfl-XuP6xPRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}